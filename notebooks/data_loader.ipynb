{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of files in train vs val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "num_files: 1460\n",
      "total_time_steps: 1460\n",
      "total_days: 365\n",
      "total_samples: 1458\n",
      "total_batches: 365\n"
     ]
    }
   ],
   "source": [
    "samples_dir = \"./data/era5_uk/samples/train\"\n",
    "num_files = len(os.listdir(samples_dir))\n",
    "total_days = 31 * 7 + 28 + 30 * 4 # Jan, Mar, May, Jul, Aug, Oct, Dec\n",
    "\n",
    "total_time_steps = total_days * 4\n",
    "pred_length = 1\n",
    "sample_length = pred_length + 2\n",
    "total_samples = total_time_steps - sample_length + 1\n",
    "total_batches = math.ceil(total_samples / 4) \n",
    "\n",
    "print(\"Training:\")\n",
    "print(\"num_files:\", num_files)\n",
    "print(\"total_time_steps:\", total_time_steps)\n",
    "print(\"total_days:\", total_days)\n",
    "print(\"total_samples:\", total_samples)\n",
    "print(\"total_batches:\", total_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10', '04', '01', '07']\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Finding the validation files \n",
    "\n",
    "samples_dir = \"./data/era5_uk/samples/val\"\n",
    "dirs = os.listdir(samples_dir)\n",
    "num_files = len(dirs)\n",
    "print(dirs)\n",
    "print(num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:\n",
      "total_files: 492\n",
      "total_time_steps: 492\n",
      "total_days: 123\n",
      "total_samples: 463\n"
     ]
    }
   ],
   "source": [
    "# Verify number of validation samples\n",
    "total_files = 0\n",
    "for m in ['01', '04', '07', '10']:\n",
    "    total_files += len(os.listdir(os.path.join(samples_dir, m)))\n",
    "    \n",
    "\n",
    "total_days = 31 + 30 + 31 + 31\n",
    "total_time_steps = total_days * 4\n",
    "pred_length = 28\n",
    "sample_length = pred_length + 2\n",
    "total_samples = total_time_steps - sample_length + 1\n",
    "total_batches = math.ceil(total_samples / 4) \n",
    "\n",
    "print(\"Validation:\")\n",
    "print(\"total_files:\", total_files)\n",
    "print(\"total_time_steps:\", total_time_steps)\n",
    "print(\"total_days:\", total_days)\n",
    "print(\"total_samples:\", total_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_batches: 94\n"
     ]
    }
   ],
   "source": [
    "def steps(total_days):\n",
    "    total_time_steps = total_days * 4\n",
    "    pred_length = 28\n",
    "    sample_length = pred_length + 2\n",
    "    total_samples = total_time_steps - sample_length + 1\n",
    "    return total_samples\n",
    "\n",
    "days = [31, 30, 31, 31]\n",
    "total_samples = sum([steps(d) for d in days])\n",
    "total_batches = math.ceil(total_samples / 4)\n",
    "print(\"total_batches:\", total_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from neural_lam.era5_dataset import ERA5UKDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_batches: 365\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"era5_uk\"\n",
    "batch_size = 4\n",
    "n_workers = 4\n",
    "\n",
    "train_set = ERA5UKDataset(\n",
    "    dataset_name,\n",
    "    pred_length=1,\n",
    "    split=\"train\",\n",
    "    standardize=False,\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=n_workers,\n",
    ")\n",
    "\n",
    "print(\"training_batches:\", len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 2.0544e+05,  1.3444e+05,  1.0096e+05,  ..., -2.0289e-01,\n",
       "           -1.5528e-02,  3.0334e-02],\n",
       "          [ 2.0545e+05,  1.3444e+05,  1.0093e+05,  ..., -1.7996e-01,\n",
       "           -8.8169e-03,  3.5368e-02],\n",
       "          [ 2.0545e+05,  1.3444e+05,  1.0091e+05,  ..., -1.5256e-01,\n",
       "            2.0826e-02,  3.8724e-02],\n",
       "          ...,\n",
       "          [ 2.0565e+05,  1.3685e+05,  1.0477e+05,  ...,  2.5573e-01,\n",
       "           -2.7840e-01, -6.5797e-03],\n",
       "          [ 2.0565e+05,  1.3686e+05,  1.0476e+05,  ...,  1.0696e-01,\n",
       "           -2.9741e-01, -2.0562e-02],\n",
       "          [ 2.0564e+05,  1.3686e+05,  1.0475e+05,  ..., -2.2974e-01,\n",
       "           -4.0424e-01,  2.0826e-02]],\n",
       "\n",
       "         [[ 2.0540e+05,  1.3449e+05,  1.0143e+05,  ...,  4.9909e-02,\n",
       "            8.9620e-02,  7.0044e-02],\n",
       "          [ 2.0541e+05,  1.3449e+05,  1.0142e+05,  ...,  3.1453e-02,\n",
       "            9.6891e-02,  6.8926e-02],\n",
       "          [ 2.0541e+05,  1.3449e+05,  1.0139e+05,  ...,  1.1877e-02,\n",
       "            1.0640e-01,  6.6688e-02],\n",
       "          ...,\n",
       "          [ 2.0548e+05,  1.3676e+05,  1.0471e+05,  ...,  1.7519e-01,\n",
       "            3.1334e-01,  3.3690e-02],\n",
       "          [ 2.0549e+05,  1.3678e+05,  1.0472e+05,  ...,  2.1546e-01,\n",
       "            2.9097e-01,  2.5300e-02],\n",
       "          [ 2.0549e+05,  1.3680e+05,  1.0472e+05,  ...,  1.7184e-01,\n",
       "            2.5629e-01,  3.8724e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 2.0585e+05,  1.3510e+05,  1.0183e+05,  ...,  1.8284e-01,\n",
       "            1.4505e-01, -1.8661e-03],\n",
       "          [ 2.0585e+05,  1.3509e+05,  1.0183e+05,  ...,  1.5694e-01,\n",
       "            1.3401e-01, -2.7153e-03],\n",
       "          [ 2.0585e+05,  1.3508e+05,  1.0182e+05,  ...,  1.2934e-01,\n",
       "            1.2637e-01,  1.0023e-02],\n",
       "          ...,\n",
       "          [ 2.0522e+05,  1.3695e+05,  1.0511e+05,  ..., -7.2582e-01,\n",
       "           -7.6743e-01,  3.6773e-02],\n",
       "          [ 2.0522e+05,  1.3695e+05,  1.0512e+05,  ..., -4.1968e-01,\n",
       "           -5.0927e-01,  9.1972e-02],\n",
       "          [ 2.0521e+05,  1.3695e+05,  1.0513e+05,  ..., -5.2626e-01,\n",
       "           -5.2711e-01,  7.3290e-02]],\n",
       "\n",
       "         [[ 2.0559e+05,  1.3500e+05,  1.0176e+05,  ...,  8.7302e-02,\n",
       "            2.1256e-01,  2.5309e-02],\n",
       "          [ 2.0559e+05,  1.3499e+05,  1.0174e+05,  ...,  9.9615e-02,\n",
       "            2.0577e-01,  2.4460e-02],\n",
       "          [ 2.0559e+05,  1.3497e+05,  1.0172e+05,  ...,  1.0981e-01,\n",
       "            2.0195e-01,  2.9980e-02],\n",
       "          ...,\n",
       "          [ 2.0508e+05,  1.3669e+05,  1.0462e+05,  ..., -4.8762e-01,\n",
       "           -1.3477e-01,  6.0127e-02],\n",
       "          [ 2.0508e+05,  1.3671e+05,  1.0468e+05,  ..., -5.9335e-01,\n",
       "            2.1935e-01,  1.3486e-01],\n",
       "          [ 2.0509e+05,  1.3674e+05,  1.0475e+05,  ..., -6.5406e-01,\n",
       "            3.3485e-01,  8.0508e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 2.0576e+05,  1.3478e+05,  1.0209e+05,  ...,  4.8232e-02,\n",
       "           -1.8325e-02, -7.1390e-03],\n",
       "          [ 2.0576e+05,  1.3480e+05,  1.0211e+05,  ...,  4.8232e-02,\n",
       "           -1.8325e-02, -7.1390e-03],\n",
       "          [ 2.0577e+05,  1.3482e+05,  1.0214e+05,  ...,  5.1028e-02,\n",
       "           -1.6088e-02, -1.3851e-02],\n",
       "          ...,\n",
       "          [ 2.0603e+05,  1.3785e+05,  1.0562e+05,  ...,  9.5772e-02,\n",
       "           -2.4428e-01,  5.3265e-02],\n",
       "          [ 2.0603e+05,  1.3784e+05,  1.0561e+05,  ...,  1.4723e-01,\n",
       "           -1.8947e-01,  1.2038e-01],\n",
       "          [ 2.0602e+05,  1.3782e+05,  1.0560e+05,  ...,  2.4958e-01,\n",
       "           -5.0205e-02,  9.0738e-02]],\n",
       "\n",
       "         [[ 2.0580e+05,  1.3481e+05,  1.0223e+05,  ...,  4.0470e-03,\n",
       "            1.9148e-02, -3.8460e-02],\n",
       "          [ 2.0580e+05,  1.3482e+05,  1.0226e+05,  ...,  1.0759e-02,\n",
       "            1.6352e-02, -3.3985e-02],\n",
       "          [ 2.0580e+05,  1.3484e+05,  1.0227e+05,  ...,  1.7470e-02,\n",
       "            1.2996e-02, -3.1189e-02],\n",
       "          ...,\n",
       "          [ 2.0624e+05,  1.3778e+05,  1.0556e+05,  ..., -1.4193e-01,\n",
       "            9.6891e-02,  5.7740e-02],\n",
       "          [ 2.0623e+05,  1.3778e+05,  1.0555e+05,  ..., -8.9915e-02,\n",
       "            2.9284e-03,  1.0752e-01],\n",
       "          [ 2.0623e+05,  1.3778e+05,  1.0554e+05,  ...,  2.5300e-02,\n",
       "           -2.0289e-01,  3.3131e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 2.0268e+05,  1.3338e+05,  1.0099e+05,  ..., -3.0686e-02,\n",
       "           -1.3791e-01, -2.9269e-02],\n",
       "          [ 2.0267e+05,  1.3337e+05,  1.0098e+05,  ..., -7.9812e-02,\n",
       "           -6.8003e-02, -4.4384e-02],\n",
       "          [ 2.0266e+05,  1.3336e+05,  1.0096e+05,  ..., -1.1713e-01,\n",
       "            1.1355e-02, -4.9108e-02],\n",
       "          ...,\n",
       "          [ 2.0055e+05,  1.3425e+05,  1.0234e+05,  ...,  8.0484e-03,\n",
       "           -4.2023e-02,  5.2923e-02],\n",
       "          [ 2.0052e+05,  1.3423e+05,  1.0235e+05,  ...,  1.7496e-02,\n",
       "           -3.7299e-02,  1.1480e-01],\n",
       "          [ 2.0050e+05,  1.3422e+05,  1.0235e+05,  ...,  4.1587e-02,\n",
       "           -5.4304e-02,  8.2683e-02]],\n",
       "\n",
       "         [[ 2.0296e+05,  1.3371e+05,  1.0156e+05,  ..., -1.4153e-02,\n",
       "            9.8743e-02, -1.0846e-02],\n",
       "          [ 2.0295e+05,  1.3371e+05,  1.0156e+05,  ..., -1.9821e-02,\n",
       "            9.6381e-02, -1.6515e-02],\n",
       "          [ 2.0295e+05,  1.3371e+05,  1.0155e+05,  ...,  3.3247e-03,\n",
       "            1.2472e-01, -1.3989e-03],\n",
       "          ...,\n",
       "          [ 2.0077e+05,  1.3450e+05,  1.0256e+05,  ...,  6.6313e-03,\n",
       "            2.7777e-01,  5.5285e-02],\n",
       "          [ 2.0073e+05,  1.3447e+05,  1.0256e+05,  ...,  7.4652e-02,\n",
       "            3.1462e-01,  8.0793e-02],\n",
       "          [ 2.0070e+05,  1.3446e+05,  1.0257e+05,  ...,  6.7094e-02,\n",
       "            1.6535e-01,  1.0063e-01]]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "init_states, target_states, forcing = next(dataiter)\n",
    "init_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_set batches: 94\n"
     ]
    }
   ],
   "source": [
    "val_set = ERA5UKDataset(\n",
    "    dataset_name,\n",
    "    pred_length=28,\n",
    "    split=\"val\",\n",
    "    standardize=False,\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_set,\n",
    "    batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=n_workers,\n",
    ")\n",
    "\n",
    "print(\"validation_set batches:\", len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_states, target_states, forcing = val_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_states: torch.Size([2, 3705, 48])\n",
      "target_states: torch.Size([28, 3705, 48])\n",
      "forcing: torch.Size([28, 3705, 12])\n"
     ]
    }
   ],
   "source": [
    "print(\"init_states:\", init_states.shape)\n",
    "print(\"target_states:\", target_states.shape)\n",
    "print(\"forcing:\", forcing.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Multi Time Resolution Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from neural_lam.era5_dataset import ERA5MultiTimeDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_steps = [2, 1]\n",
    "dataset_name = \"era5_uk\"\n",
    "pred_length = 28\n",
    "train_set = ERA5MultiTimeDataset(\n",
    "    dataset_name,\n",
    "    subsample_steps=subsample_steps,\n",
    "    split=\"train\",\n",
    ")\n",
    "\n",
    "batch_size = 4\n",
    "n_workers = 4\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=n_workers,\n",
    ")\n",
    "dataiter = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_states, target_states, forcing = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1401\n",
      "1431\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set.datasets[0]))\n",
    "print(len(train_set.datasets[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460\n",
      "1460\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set.datasets[0].sample_names))\n",
    "print(len(train_set.datasets[1].sample_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "pred_length = 8\n",
    "max_subsample_step = 2\n",
    "sample_length = pred_length + 2 * max_subsample_step\n",
    "\n",
    "total_step_samples = 50\n",
    "total_samples = total_step_samples - sample_length + 1\n",
    "\n",
    "print(sample_length)\n",
    "print(total_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m subsample_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      5\u001b[0m sample_length \u001b[38;5;241m=\u001b[39m (pred_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m subsample_step\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sample_length \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m n_samples\n\u001b[1;32m      9\u001b[0m items \u001b[38;5;241m=\u001b[39m n_samples \u001b[38;5;241m-\u001b[39m sample_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitems:\u001b[39m\u001b[38;5;124m\"\u001b[39m, items)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_samples = 12\n",
    "pred_length = 8\n",
    "subsample_step = 2\n",
    "\n",
    "sample_length = (pred_length + 2) * subsample_step\n",
    "assert sample_length <= n_samples\n",
    "\n",
    "\n",
    "items = n_samples - sample_length + 1\n",
    "print(\"items:\", items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20220101000000.npy',\n",
       " '20220101060000.npy',\n",
       " '20220101120000.npy',\n",
       " '20220101180000.npy',\n",
       " '20220102000000.npy',\n",
       " '20220102060000.npy',\n",
       " '20220102120000.npy',\n",
       " '20220102180000.npy',\n",
       " '20220103000000.npy',\n",
       " '20220103060000.npy',\n",
       " '20220103120000.npy',\n",
       " '20220103180000.npy',\n",
       " '20220104000000.npy',\n",
       " '20220104060000.npy',\n",
       " '20220104120000.npy',\n",
       " '20220104180000.npy',\n",
       " '20220105000000.npy',\n",
       " '20220105060000.npy',\n",
       " '20220105120000.npy',\n",
       " '20220105180000.npy',\n",
       " '20220106000000.npy',\n",
       " '20220106060000.npy',\n",
       " '20220106120000.npy',\n",
       " '20220106180000.npy',\n",
       " '20220107000000.npy',\n",
       " '20220107060000.npy',\n",
       " '20220107120000.npy',\n",
       " '20220107180000.npy',\n",
       " '20220108000000.npy',\n",
       " '20220108060000.npy',\n",
       " '20220108120000.npy',\n",
       " '20220108180000.npy',\n",
       " '20220109000000.npy',\n",
       " '20220109060000.npy',\n",
       " '20220109120000.npy',\n",
       " '20220109180000.npy',\n",
       " '20220110000000.npy',\n",
       " '20220110060000.npy',\n",
       " '20220110120000.npy',\n",
       " '20220110180000.npy',\n",
       " '20220111000000.npy',\n",
       " '20220111060000.npy',\n",
       " '20220111120000.npy',\n",
       " '20220111180000.npy',\n",
       " '20220112000000.npy',\n",
       " '20220112060000.npy',\n",
       " '20220112120000.npy',\n",
       " '20220112180000.npy',\n",
       " '20220113000000.npy',\n",
       " '20220113060000.npy',\n",
       " '20220113120000.npy',\n",
       " '20220113180000.npy',\n",
       " '20220114000000.npy',\n",
       " '20220114060000.npy',\n",
       " '20220114120000.npy',\n",
       " '20220114180000.npy',\n",
       " '20220115000000.npy',\n",
       " '20220115060000.npy',\n",
       " '20220115120000.npy',\n",
       " '20220115180000.npy',\n",
       " '20220116000000.npy',\n",
       " '20220116060000.npy',\n",
       " '20220116120000.npy',\n",
       " '20220116180000.npy',\n",
       " '20220117000000.npy',\n",
       " '20220117060000.npy',\n",
       " '20220117120000.npy',\n",
       " '20220117180000.npy',\n",
       " '20220118000000.npy',\n",
       " '20220118060000.npy',\n",
       " '20220118120000.npy',\n",
       " '20220118180000.npy',\n",
       " '20220119000000.npy',\n",
       " '20220119060000.npy',\n",
       " '20220119120000.npy',\n",
       " '20220119180000.npy',\n",
       " '20220120000000.npy',\n",
       " '20220120060000.npy',\n",
       " '20220120120000.npy',\n",
       " '20220120180000.npy',\n",
       " '20220121000000.npy',\n",
       " '20220121060000.npy',\n",
       " '20220121120000.npy',\n",
       " '20220121180000.npy',\n",
       " '20220122000000.npy',\n",
       " '20220122060000.npy',\n",
       " '20220122120000.npy',\n",
       " '20220122180000.npy',\n",
       " '20220123000000.npy',\n",
       " '20220123060000.npy',\n",
       " '20220123120000.npy',\n",
       " '20220123180000.npy',\n",
       " '20220124000000.npy',\n",
       " '20220124060000.npy',\n",
       " '20220124120000.npy',\n",
       " '20220124180000.npy',\n",
       " '20220125000000.npy',\n",
       " '20220125060000.npy',\n",
       " '20220125120000.npy',\n",
       " '20220125180000.npy',\n",
       " '20220126000000.npy',\n",
       " '20220126060000.npy',\n",
       " '20220126120000.npy',\n",
       " '20220126180000.npy',\n",
       " '20220127000000.npy',\n",
       " '20220127060000.npy',\n",
       " '20220127120000.npy',\n",
       " '20220127180000.npy',\n",
       " '20220128000000.npy',\n",
       " '20220128060000.npy',\n",
       " '20220128120000.npy',\n",
       " '20220128180000.npy',\n",
       " '20220129000000.npy',\n",
       " '20220129060000.npy',\n",
       " '20220129120000.npy',\n",
       " '20220129180000.npy',\n",
       " '20220130000000.npy',\n",
       " '20220130060000.npy',\n",
       " '20220130120000.npy',\n",
       " '20220130180000.npy',\n",
       " '20220131000000.npy',\n",
       " '20220131060000.npy',\n",
       " '20220131120000.npy',\n",
       " '20220131180000.npy',\n",
       " '20220201000000.npy',\n",
       " '20220201060000.npy',\n",
       " '20220201120000.npy',\n",
       " '20220201180000.npy',\n",
       " '20220202000000.npy',\n",
       " '20220202060000.npy',\n",
       " '20220202120000.npy',\n",
       " '20220202180000.npy',\n",
       " '20220203000000.npy',\n",
       " '20220203060000.npy',\n",
       " '20220203120000.npy',\n",
       " '20220203180000.npy',\n",
       " '20220204000000.npy',\n",
       " '20220204060000.npy',\n",
       " '20220204120000.npy',\n",
       " '20220204180000.npy',\n",
       " '20220205000000.npy',\n",
       " '20220205060000.npy',\n",
       " '20220205120000.npy',\n",
       " '20220205180000.npy',\n",
       " '20220206000000.npy',\n",
       " '20220206060000.npy',\n",
       " '20220206120000.npy',\n",
       " '20220206180000.npy',\n",
       " '20220207000000.npy',\n",
       " '20220207060000.npy',\n",
       " '20220207120000.npy',\n",
       " '20220207180000.npy',\n",
       " '20220208000000.npy',\n",
       " '20220208060000.npy',\n",
       " '20220208120000.npy',\n",
       " '20220208180000.npy',\n",
       " '20220209000000.npy',\n",
       " '20220209060000.npy',\n",
       " '20220209120000.npy',\n",
       " '20220209180000.npy',\n",
       " '20220210000000.npy',\n",
       " '20220210060000.npy',\n",
       " '20220210120000.npy',\n",
       " '20220210180000.npy',\n",
       " '20220211000000.npy',\n",
       " '20220211060000.npy',\n",
       " '20220211120000.npy',\n",
       " '20220211180000.npy',\n",
       " '20220212000000.npy',\n",
       " '20220212060000.npy',\n",
       " '20220212120000.npy',\n",
       " '20220212180000.npy',\n",
       " '20220213000000.npy',\n",
       " '20220213060000.npy',\n",
       " '20220213120000.npy',\n",
       " '20220213180000.npy',\n",
       " '20220214000000.npy',\n",
       " '20220214060000.npy',\n",
       " '20220214120000.npy',\n",
       " '20220214180000.npy',\n",
       " '20220215000000.npy',\n",
       " '20220215060000.npy',\n",
       " '20220215120000.npy',\n",
       " '20220215180000.npy',\n",
       " '20220216000000.npy',\n",
       " '20220216060000.npy',\n",
       " '20220216120000.npy',\n",
       " '20220216180000.npy',\n",
       " '20220217000000.npy',\n",
       " '20220217060000.npy',\n",
       " '20220217120000.npy',\n",
       " '20220217180000.npy',\n",
       " '20220218000000.npy',\n",
       " '20220218060000.npy',\n",
       " '20220218120000.npy',\n",
       " '20220218180000.npy',\n",
       " '20220219000000.npy',\n",
       " '20220219060000.npy',\n",
       " '20220219120000.npy',\n",
       " '20220219180000.npy',\n",
       " '20220220000000.npy',\n",
       " '20220220060000.npy',\n",
       " '20220220120000.npy',\n",
       " '20220220180000.npy',\n",
       " '20220221000000.npy',\n",
       " '20220221060000.npy',\n",
       " '20220221120000.npy',\n",
       " '20220221180000.npy',\n",
       " '20220222000000.npy',\n",
       " '20220222060000.npy',\n",
       " '20220222120000.npy',\n",
       " '20220222180000.npy',\n",
       " '20220223000000.npy',\n",
       " '20220223060000.npy',\n",
       " '20220223120000.npy',\n",
       " '20220223180000.npy',\n",
       " '20220224000000.npy',\n",
       " '20220224060000.npy',\n",
       " '20220224120000.npy',\n",
       " '20220224180000.npy',\n",
       " '20220225000000.npy',\n",
       " '20220225060000.npy',\n",
       " '20220225120000.npy',\n",
       " '20220225180000.npy',\n",
       " '20220226000000.npy',\n",
       " '20220226060000.npy',\n",
       " '20220226120000.npy',\n",
       " '20220226180000.npy',\n",
       " '20220227000000.npy',\n",
       " '20220227060000.npy',\n",
       " '20220227120000.npy',\n",
       " '20220227180000.npy',\n",
       " '20220228000000.npy',\n",
       " '20220228060000.npy',\n",
       " '20220228120000.npy',\n",
       " '20220228180000.npy',\n",
       " '20220301000000.npy',\n",
       " '20220301060000.npy',\n",
       " '20220301120000.npy',\n",
       " '20220301180000.npy',\n",
       " '20220302000000.npy',\n",
       " '20220302060000.npy',\n",
       " '20220302120000.npy',\n",
       " '20220302180000.npy',\n",
       " '20220303000000.npy',\n",
       " '20220303060000.npy',\n",
       " '20220303120000.npy',\n",
       " '20220303180000.npy',\n",
       " '20220304000000.npy',\n",
       " '20220304060000.npy',\n",
       " '20220304120000.npy',\n",
       " '20220304180000.npy',\n",
       " '20220305000000.npy',\n",
       " '20220305060000.npy',\n",
       " '20220305120000.npy',\n",
       " '20220305180000.npy',\n",
       " '20220306000000.npy',\n",
       " '20220306060000.npy',\n",
       " '20220306120000.npy',\n",
       " '20220306180000.npy',\n",
       " '20220307000000.npy',\n",
       " '20220307060000.npy',\n",
       " '20220307120000.npy',\n",
       " '20220307180000.npy',\n",
       " '20220308000000.npy',\n",
       " '20220308060000.npy',\n",
       " '20220308120000.npy',\n",
       " '20220308180000.npy',\n",
       " '20220309000000.npy',\n",
       " '20220309060000.npy',\n",
       " '20220309120000.npy',\n",
       " '20220309180000.npy',\n",
       " '20220310000000.npy',\n",
       " '20220310060000.npy',\n",
       " '20220310120000.npy',\n",
       " '20220310180000.npy',\n",
       " '20220311000000.npy',\n",
       " '20220311060000.npy',\n",
       " '20220311120000.npy',\n",
       " '20220311180000.npy',\n",
       " '20220312000000.npy',\n",
       " '20220312060000.npy',\n",
       " '20220312120000.npy',\n",
       " '20220312180000.npy',\n",
       " '20220313000000.npy',\n",
       " '20220313060000.npy',\n",
       " '20220313120000.npy',\n",
       " '20220313180000.npy',\n",
       " '20220314000000.npy',\n",
       " '20220314060000.npy',\n",
       " '20220314120000.npy',\n",
       " '20220314180000.npy',\n",
       " '20220315000000.npy',\n",
       " '20220315060000.npy',\n",
       " '20220315120000.npy',\n",
       " '20220315180000.npy',\n",
       " '20220316000000.npy',\n",
       " '20220316060000.npy',\n",
       " '20220316120000.npy',\n",
       " '20220316180000.npy',\n",
       " '20220317000000.npy',\n",
       " '20220317060000.npy',\n",
       " '20220317120000.npy',\n",
       " '20220317180000.npy',\n",
       " '20220318000000.npy',\n",
       " '20220318060000.npy',\n",
       " '20220318120000.npy',\n",
       " '20220318180000.npy',\n",
       " '20220319000000.npy',\n",
       " '20220319060000.npy',\n",
       " '20220319120000.npy',\n",
       " '20220319180000.npy',\n",
       " '20220320000000.npy',\n",
       " '20220320060000.npy',\n",
       " '20220320120000.npy',\n",
       " '20220320180000.npy',\n",
       " '20220321000000.npy',\n",
       " '20220321060000.npy',\n",
       " '20220321120000.npy',\n",
       " '20220321180000.npy',\n",
       " '20220322000000.npy',\n",
       " '20220322060000.npy',\n",
       " '20220322120000.npy',\n",
       " '20220322180000.npy',\n",
       " '20220323000000.npy',\n",
       " '20220323060000.npy',\n",
       " '20220323120000.npy',\n",
       " '20220323180000.npy',\n",
       " '20220324000000.npy',\n",
       " '20220324060000.npy',\n",
       " '20220324120000.npy',\n",
       " '20220324180000.npy',\n",
       " '20220325000000.npy',\n",
       " '20220325060000.npy',\n",
       " '20220325120000.npy',\n",
       " '20220325180000.npy',\n",
       " '20220326000000.npy',\n",
       " '20220326060000.npy',\n",
       " '20220326120000.npy',\n",
       " '20220326180000.npy',\n",
       " '20220327000000.npy',\n",
       " '20220327060000.npy',\n",
       " '20220327120000.npy',\n",
       " '20220327180000.npy',\n",
       " '20220328000000.npy',\n",
       " '20220328060000.npy',\n",
       " '20220328120000.npy',\n",
       " '20220328180000.npy',\n",
       " '20220329000000.npy',\n",
       " '20220329060000.npy',\n",
       " '20220329120000.npy',\n",
       " '20220329180000.npy',\n",
       " '20220330000000.npy',\n",
       " '20220330060000.npy',\n",
       " '20220330120000.npy',\n",
       " '20220330180000.npy',\n",
       " '20220331000000.npy',\n",
       " '20220331060000.npy',\n",
       " '20220331120000.npy',\n",
       " '20220331180000.npy',\n",
       " '20220401000000.npy',\n",
       " '20220401060000.npy',\n",
       " '20220401120000.npy',\n",
       " '20220401180000.npy',\n",
       " '20220402000000.npy',\n",
       " '20220402060000.npy',\n",
       " '20220402120000.npy',\n",
       " '20220402180000.npy',\n",
       " '20220403000000.npy',\n",
       " '20220403060000.npy',\n",
       " '20220403120000.npy',\n",
       " '20220403180000.npy',\n",
       " '20220404000000.npy',\n",
       " '20220404060000.npy',\n",
       " '20220404120000.npy',\n",
       " '20220404180000.npy',\n",
       " '20220405000000.npy',\n",
       " '20220405060000.npy',\n",
       " '20220405120000.npy',\n",
       " '20220405180000.npy',\n",
       " '20220406000000.npy',\n",
       " '20220406060000.npy',\n",
       " '20220406120000.npy',\n",
       " '20220406180000.npy',\n",
       " '20220407000000.npy',\n",
       " '20220407060000.npy',\n",
       " '20220407120000.npy',\n",
       " '20220407180000.npy',\n",
       " '20220408000000.npy',\n",
       " '20220408060000.npy',\n",
       " '20220408120000.npy',\n",
       " '20220408180000.npy',\n",
       " '20220409000000.npy',\n",
       " '20220409060000.npy',\n",
       " '20220409120000.npy',\n",
       " '20220409180000.npy',\n",
       " '20220410000000.npy',\n",
       " '20220410060000.npy',\n",
       " '20220410120000.npy',\n",
       " '20220410180000.npy',\n",
       " '20220411000000.npy',\n",
       " '20220411060000.npy',\n",
       " '20220411120000.npy',\n",
       " '20220411180000.npy',\n",
       " '20220412000000.npy',\n",
       " '20220412060000.npy',\n",
       " '20220412120000.npy',\n",
       " '20220412180000.npy',\n",
       " '20220413000000.npy',\n",
       " '20220413060000.npy',\n",
       " '20220413120000.npy',\n",
       " '20220413180000.npy',\n",
       " '20220414000000.npy',\n",
       " '20220414060000.npy',\n",
       " '20220414120000.npy',\n",
       " '20220414180000.npy',\n",
       " '20220415000000.npy',\n",
       " '20220415060000.npy',\n",
       " '20220415120000.npy',\n",
       " '20220415180000.npy',\n",
       " '20220416000000.npy',\n",
       " '20220416060000.npy',\n",
       " '20220416120000.npy',\n",
       " '20220416180000.npy',\n",
       " '20220417000000.npy',\n",
       " '20220417060000.npy',\n",
       " '20220417120000.npy',\n",
       " '20220417180000.npy',\n",
       " '20220418000000.npy',\n",
       " '20220418060000.npy',\n",
       " '20220418120000.npy',\n",
       " '20220418180000.npy',\n",
       " '20220419000000.npy',\n",
       " '20220419060000.npy',\n",
       " '20220419120000.npy',\n",
       " '20220419180000.npy',\n",
       " '20220420000000.npy',\n",
       " '20220420060000.npy',\n",
       " '20220420120000.npy',\n",
       " '20220420180000.npy',\n",
       " '20220421000000.npy',\n",
       " '20220421060000.npy',\n",
       " '20220421120000.npy',\n",
       " '20220421180000.npy',\n",
       " '20220422000000.npy',\n",
       " '20220422060000.npy',\n",
       " '20220422120000.npy',\n",
       " '20220422180000.npy',\n",
       " '20220423000000.npy',\n",
       " '20220423060000.npy',\n",
       " '20220423120000.npy',\n",
       " '20220423180000.npy',\n",
       " '20220424000000.npy',\n",
       " '20220424060000.npy',\n",
       " '20220424120000.npy',\n",
       " '20220424180000.npy',\n",
       " '20220425000000.npy',\n",
       " '20220425060000.npy',\n",
       " '20220425120000.npy',\n",
       " '20220425180000.npy',\n",
       " '20220426000000.npy',\n",
       " '20220426060000.npy',\n",
       " '20220426120000.npy',\n",
       " '20220426180000.npy',\n",
       " '20220427000000.npy',\n",
       " '20220427060000.npy',\n",
       " '20220427120000.npy',\n",
       " '20220427180000.npy',\n",
       " '20220428000000.npy',\n",
       " '20220428060000.npy',\n",
       " '20220428120000.npy',\n",
       " '20220428180000.npy',\n",
       " '20220429000000.npy',\n",
       " '20220429060000.npy',\n",
       " '20220429120000.npy',\n",
       " '20220429180000.npy',\n",
       " '20220430000000.npy',\n",
       " '20220430060000.npy',\n",
       " '20220430120000.npy',\n",
       " '20220430180000.npy',\n",
       " '20220501000000.npy',\n",
       " '20220501060000.npy',\n",
       " '20220501120000.npy',\n",
       " '20220501180000.npy',\n",
       " '20220502000000.npy',\n",
       " '20220502060000.npy',\n",
       " '20220502120000.npy',\n",
       " '20220502180000.npy',\n",
       " '20220503000000.npy',\n",
       " '20220503060000.npy',\n",
       " '20220503120000.npy',\n",
       " '20220503180000.npy',\n",
       " '20220504000000.npy',\n",
       " '20220504060000.npy',\n",
       " '20220504120000.npy',\n",
       " '20220504180000.npy',\n",
       " '20220505000000.npy',\n",
       " '20220505060000.npy',\n",
       " '20220505120000.npy',\n",
       " '20220505180000.npy',\n",
       " '20220506000000.npy',\n",
       " '20220506060000.npy',\n",
       " '20220506120000.npy',\n",
       " '20220506180000.npy',\n",
       " '20220507000000.npy',\n",
       " '20220507060000.npy',\n",
       " '20220507120000.npy',\n",
       " '20220507180000.npy',\n",
       " '20220508000000.npy',\n",
       " '20220508060000.npy',\n",
       " '20220508120000.npy',\n",
       " '20220508180000.npy',\n",
       " '20220509000000.npy',\n",
       " '20220509060000.npy',\n",
       " '20220509120000.npy',\n",
       " '20220509180000.npy',\n",
       " '20220510000000.npy',\n",
       " '20220510060000.npy',\n",
       " '20220510120000.npy',\n",
       " '20220510180000.npy',\n",
       " '20220511000000.npy',\n",
       " '20220511060000.npy',\n",
       " '20220511120000.npy',\n",
       " '20220511180000.npy',\n",
       " '20220512000000.npy',\n",
       " '20220512060000.npy',\n",
       " '20220512120000.npy',\n",
       " '20220512180000.npy',\n",
       " '20220513000000.npy',\n",
       " '20220513060000.npy',\n",
       " '20220513120000.npy',\n",
       " '20220513180000.npy',\n",
       " '20220514000000.npy',\n",
       " '20220514060000.npy',\n",
       " '20220514120000.npy',\n",
       " '20220514180000.npy',\n",
       " '20220515000000.npy',\n",
       " '20220515060000.npy',\n",
       " '20220515120000.npy',\n",
       " '20220515180000.npy',\n",
       " '20220516000000.npy',\n",
       " '20220516060000.npy',\n",
       " '20220516120000.npy',\n",
       " '20220516180000.npy',\n",
       " '20220517000000.npy',\n",
       " '20220517060000.npy',\n",
       " '20220517120000.npy',\n",
       " '20220517180000.npy',\n",
       " '20220518000000.npy',\n",
       " '20220518060000.npy',\n",
       " '20220518120000.npy',\n",
       " '20220518180000.npy',\n",
       " '20220519000000.npy',\n",
       " '20220519060000.npy',\n",
       " '20220519120000.npy',\n",
       " '20220519180000.npy',\n",
       " '20220520000000.npy',\n",
       " '20220520060000.npy',\n",
       " '20220520120000.npy',\n",
       " '20220520180000.npy',\n",
       " '20220521000000.npy',\n",
       " '20220521060000.npy',\n",
       " '20220521120000.npy',\n",
       " '20220521180000.npy',\n",
       " '20220522000000.npy',\n",
       " '20220522060000.npy',\n",
       " '20220522120000.npy',\n",
       " '20220522180000.npy',\n",
       " '20220523000000.npy',\n",
       " '20220523060000.npy',\n",
       " '20220523120000.npy',\n",
       " '20220523180000.npy',\n",
       " '20220524000000.npy',\n",
       " '20220524060000.npy',\n",
       " '20220524120000.npy',\n",
       " '20220524180000.npy',\n",
       " '20220525000000.npy',\n",
       " '20220525060000.npy',\n",
       " '20220525120000.npy',\n",
       " '20220525180000.npy',\n",
       " '20220526000000.npy',\n",
       " '20220526060000.npy',\n",
       " '20220526120000.npy',\n",
       " '20220526180000.npy',\n",
       " '20220527000000.npy',\n",
       " '20220527060000.npy',\n",
       " '20220527120000.npy',\n",
       " '20220527180000.npy',\n",
       " '20220528000000.npy',\n",
       " '20220528060000.npy',\n",
       " '20220528120000.npy',\n",
       " '20220528180000.npy',\n",
       " '20220529000000.npy',\n",
       " '20220529060000.npy',\n",
       " '20220529120000.npy',\n",
       " '20220529180000.npy',\n",
       " '20220530000000.npy',\n",
       " '20220530060000.npy',\n",
       " '20220530120000.npy',\n",
       " '20220530180000.npy',\n",
       " '20220531000000.npy',\n",
       " '20220531060000.npy',\n",
       " '20220531120000.npy',\n",
       " '20220531180000.npy',\n",
       " '20220601000000.npy',\n",
       " '20220601060000.npy',\n",
       " '20220601120000.npy',\n",
       " '20220601180000.npy',\n",
       " '20220602000000.npy',\n",
       " '20220602060000.npy',\n",
       " '20220602120000.npy',\n",
       " '20220602180000.npy',\n",
       " '20220603000000.npy',\n",
       " '20220603060000.npy',\n",
       " '20220603120000.npy',\n",
       " '20220603180000.npy',\n",
       " '20220604000000.npy',\n",
       " '20220604060000.npy',\n",
       " '20220604120000.npy',\n",
       " '20220604180000.npy',\n",
       " '20220605000000.npy',\n",
       " '20220605060000.npy',\n",
       " '20220605120000.npy',\n",
       " '20220605180000.npy',\n",
       " '20220606000000.npy',\n",
       " '20220606060000.npy',\n",
       " '20220606120000.npy',\n",
       " '20220606180000.npy',\n",
       " '20220607000000.npy',\n",
       " '20220607060000.npy',\n",
       " '20220607120000.npy',\n",
       " '20220607180000.npy',\n",
       " '20220608000000.npy',\n",
       " '20220608060000.npy',\n",
       " '20220608120000.npy',\n",
       " '20220608180000.npy',\n",
       " '20220609000000.npy',\n",
       " '20220609060000.npy',\n",
       " '20220609120000.npy',\n",
       " '20220609180000.npy',\n",
       " '20220610000000.npy',\n",
       " '20220610060000.npy',\n",
       " '20220610120000.npy',\n",
       " '20220610180000.npy',\n",
       " '20220611000000.npy',\n",
       " '20220611060000.npy',\n",
       " '20220611120000.npy',\n",
       " '20220611180000.npy',\n",
       " '20220612000000.npy',\n",
       " '20220612060000.npy',\n",
       " '20220612120000.npy',\n",
       " '20220612180000.npy',\n",
       " '20220613000000.npy',\n",
       " '20220613060000.npy',\n",
       " '20220613120000.npy',\n",
       " '20220613180000.npy',\n",
       " '20220614000000.npy',\n",
       " '20220614060000.npy',\n",
       " '20220614120000.npy',\n",
       " '20220614180000.npy',\n",
       " '20220615000000.npy',\n",
       " '20220615060000.npy',\n",
       " '20220615120000.npy',\n",
       " '20220615180000.npy',\n",
       " '20220616000000.npy',\n",
       " '20220616060000.npy',\n",
       " '20220616120000.npy',\n",
       " '20220616180000.npy',\n",
       " '20220617000000.npy',\n",
       " '20220617060000.npy',\n",
       " '20220617120000.npy',\n",
       " '20220617180000.npy',\n",
       " '20220618000000.npy',\n",
       " '20220618060000.npy',\n",
       " '20220618120000.npy',\n",
       " '20220618180000.npy',\n",
       " '20220619000000.npy',\n",
       " '20220619060000.npy',\n",
       " '20220619120000.npy',\n",
       " '20220619180000.npy',\n",
       " '20220620000000.npy',\n",
       " '20220620060000.npy',\n",
       " '20220620120000.npy',\n",
       " '20220620180000.npy',\n",
       " '20220621000000.npy',\n",
       " '20220621060000.npy',\n",
       " '20220621120000.npy',\n",
       " '20220621180000.npy',\n",
       " '20220622000000.npy',\n",
       " '20220622060000.npy',\n",
       " '20220622120000.npy',\n",
       " '20220622180000.npy',\n",
       " '20220623000000.npy',\n",
       " '20220623060000.npy',\n",
       " '20220623120000.npy',\n",
       " '20220623180000.npy',\n",
       " '20220624000000.npy',\n",
       " '20220624060000.npy',\n",
       " '20220624120000.npy',\n",
       " '20220624180000.npy',\n",
       " '20220625000000.npy',\n",
       " '20220625060000.npy',\n",
       " '20220625120000.npy',\n",
       " '20220625180000.npy',\n",
       " '20220626000000.npy',\n",
       " '20220626060000.npy',\n",
       " '20220626120000.npy',\n",
       " '20220626180000.npy',\n",
       " '20220627000000.npy',\n",
       " '20220627060000.npy',\n",
       " '20220627120000.npy',\n",
       " '20220627180000.npy',\n",
       " '20220628000000.npy',\n",
       " '20220628060000.npy',\n",
       " '20220628120000.npy',\n",
       " '20220628180000.npy',\n",
       " '20220629000000.npy',\n",
       " '20220629060000.npy',\n",
       " '20220629120000.npy',\n",
       " '20220629180000.npy',\n",
       " '20220630000000.npy',\n",
       " '20220630060000.npy',\n",
       " '20220630120000.npy',\n",
       " '20220630180000.npy',\n",
       " '20220701000000.npy',\n",
       " '20220701060000.npy',\n",
       " '20220701120000.npy',\n",
       " '20220701180000.npy',\n",
       " '20220702000000.npy',\n",
       " '20220702060000.npy',\n",
       " '20220702120000.npy',\n",
       " '20220702180000.npy',\n",
       " '20220703000000.npy',\n",
       " '20220703060000.npy',\n",
       " '20220703120000.npy',\n",
       " '20220703180000.npy',\n",
       " '20220704000000.npy',\n",
       " '20220704060000.npy',\n",
       " '20220704120000.npy',\n",
       " '20220704180000.npy',\n",
       " '20220705000000.npy',\n",
       " '20220705060000.npy',\n",
       " '20220705120000.npy',\n",
       " '20220705180000.npy',\n",
       " '20220706000000.npy',\n",
       " '20220706060000.npy',\n",
       " '20220706120000.npy',\n",
       " '20220706180000.npy',\n",
       " '20220707000000.npy',\n",
       " '20220707060000.npy',\n",
       " '20220707120000.npy',\n",
       " '20220707180000.npy',\n",
       " '20220708000000.npy',\n",
       " '20220708060000.npy',\n",
       " '20220708120000.npy',\n",
       " '20220708180000.npy',\n",
       " '20220709000000.npy',\n",
       " '20220709060000.npy',\n",
       " '20220709120000.npy',\n",
       " '20220709180000.npy',\n",
       " '20220710000000.npy',\n",
       " '20220710060000.npy',\n",
       " '20220710120000.npy',\n",
       " '20220710180000.npy',\n",
       " '20220711000000.npy',\n",
       " '20220711060000.npy',\n",
       " '20220711120000.npy',\n",
       " '20220711180000.npy',\n",
       " '20220712000000.npy',\n",
       " '20220712060000.npy',\n",
       " '20220712120000.npy',\n",
       " '20220712180000.npy',\n",
       " '20220713000000.npy',\n",
       " '20220713060000.npy',\n",
       " '20220713120000.npy',\n",
       " '20220713180000.npy',\n",
       " '20220714000000.npy',\n",
       " '20220714060000.npy',\n",
       " '20220714120000.npy',\n",
       " '20220714180000.npy',\n",
       " '20220715000000.npy',\n",
       " '20220715060000.npy',\n",
       " '20220715120000.npy',\n",
       " '20220715180000.npy',\n",
       " '20220716000000.npy',\n",
       " '20220716060000.npy',\n",
       " '20220716120000.npy',\n",
       " '20220716180000.npy',\n",
       " '20220717000000.npy',\n",
       " '20220717060000.npy',\n",
       " '20220717120000.npy',\n",
       " '20220717180000.npy',\n",
       " '20220718000000.npy',\n",
       " '20220718060000.npy',\n",
       " '20220718120000.npy',\n",
       " '20220718180000.npy',\n",
       " '20220719000000.npy',\n",
       " '20220719060000.npy',\n",
       " '20220719120000.npy',\n",
       " '20220719180000.npy',\n",
       " '20220720000000.npy',\n",
       " '20220720060000.npy',\n",
       " '20220720120000.npy',\n",
       " '20220720180000.npy',\n",
       " '20220721000000.npy',\n",
       " '20220721060000.npy',\n",
       " '20220721120000.npy',\n",
       " '20220721180000.npy',\n",
       " '20220722000000.npy',\n",
       " '20220722060000.npy',\n",
       " '20220722120000.npy',\n",
       " '20220722180000.npy',\n",
       " '20220723000000.npy',\n",
       " '20220723060000.npy',\n",
       " '20220723120000.npy',\n",
       " '20220723180000.npy',\n",
       " '20220724000000.npy',\n",
       " '20220724060000.npy',\n",
       " '20220724120000.npy',\n",
       " '20220724180000.npy',\n",
       " '20220725000000.npy',\n",
       " '20220725060000.npy',\n",
       " '20220725120000.npy',\n",
       " '20220725180000.npy',\n",
       " '20220726000000.npy',\n",
       " '20220726060000.npy',\n",
       " '20220726120000.npy',\n",
       " '20220726180000.npy',\n",
       " '20220727000000.npy',\n",
       " '20220727060000.npy',\n",
       " '20220727120000.npy',\n",
       " '20220727180000.npy',\n",
       " '20220728000000.npy',\n",
       " '20220728060000.npy',\n",
       " '20220728120000.npy',\n",
       " '20220728180000.npy',\n",
       " '20220729000000.npy',\n",
       " '20220729060000.npy',\n",
       " '20220729120000.npy',\n",
       " '20220729180000.npy',\n",
       " '20220730000000.npy',\n",
       " '20220730060000.npy',\n",
       " '20220730120000.npy',\n",
       " '20220730180000.npy',\n",
       " '20220731000000.npy',\n",
       " '20220731060000.npy',\n",
       " '20220731120000.npy',\n",
       " '20220731180000.npy',\n",
       " '20220801000000.npy',\n",
       " '20220801060000.npy',\n",
       " '20220801120000.npy',\n",
       " '20220801180000.npy',\n",
       " '20220802000000.npy',\n",
       " '20220802060000.npy',\n",
       " '20220802120000.npy',\n",
       " '20220802180000.npy',\n",
       " '20220803000000.npy',\n",
       " '20220803060000.npy',\n",
       " '20220803120000.npy',\n",
       " '20220803180000.npy',\n",
       " '20220804000000.npy',\n",
       " '20220804060000.npy',\n",
       " '20220804120000.npy',\n",
       " '20220804180000.npy',\n",
       " '20220805000000.npy',\n",
       " '20220805060000.npy',\n",
       " '20220805120000.npy',\n",
       " '20220805180000.npy',\n",
       " '20220806000000.npy',\n",
       " '20220806060000.npy',\n",
       " '20220806120000.npy',\n",
       " '20220806180000.npy',\n",
       " '20220807000000.npy',\n",
       " '20220807060000.npy',\n",
       " '20220807120000.npy',\n",
       " '20220807180000.npy',\n",
       " '20220808000000.npy',\n",
       " '20220808060000.npy',\n",
       " '20220808120000.npy',\n",
       " '20220808180000.npy',\n",
       " '20220809000000.npy',\n",
       " '20220809060000.npy',\n",
       " '20220809120000.npy',\n",
       " '20220809180000.npy',\n",
       " '20220810000000.npy',\n",
       " '20220810060000.npy',\n",
       " '20220810120000.npy',\n",
       " '20220810180000.npy',\n",
       " '20220811000000.npy',\n",
       " '20220811060000.npy',\n",
       " '20220811120000.npy',\n",
       " '20220811180000.npy',\n",
       " '20220812000000.npy',\n",
       " '20220812060000.npy',\n",
       " '20220812120000.npy',\n",
       " '20220812180000.npy',\n",
       " '20220813000000.npy',\n",
       " '20220813060000.npy',\n",
       " '20220813120000.npy',\n",
       " '20220813180000.npy',\n",
       " '20220814000000.npy',\n",
       " '20220814060000.npy',\n",
       " '20220814120000.npy',\n",
       " '20220814180000.npy',\n",
       " '20220815000000.npy',\n",
       " '20220815060000.npy',\n",
       " '20220815120000.npy',\n",
       " '20220815180000.npy',\n",
       " '20220816000000.npy',\n",
       " '20220816060000.npy',\n",
       " '20220816120000.npy',\n",
       " '20220816180000.npy',\n",
       " '20220817000000.npy',\n",
       " '20220817060000.npy',\n",
       " '20220817120000.npy',\n",
       " '20220817180000.npy',\n",
       " '20220818000000.npy',\n",
       " '20220818060000.npy',\n",
       " '20220818120000.npy',\n",
       " '20220818180000.npy',\n",
       " '20220819000000.npy',\n",
       " '20220819060000.npy',\n",
       " '20220819120000.npy',\n",
       " '20220819180000.npy',\n",
       " '20220820000000.npy',\n",
       " '20220820060000.npy',\n",
       " '20220820120000.npy',\n",
       " '20220820180000.npy',\n",
       " '20220821000000.npy',\n",
       " '20220821060000.npy',\n",
       " '20220821120000.npy',\n",
       " '20220821180000.npy',\n",
       " '20220822000000.npy',\n",
       " '20220822060000.npy',\n",
       " '20220822120000.npy',\n",
       " '20220822180000.npy',\n",
       " '20220823000000.npy',\n",
       " '20220823060000.npy',\n",
       " '20220823120000.npy',\n",
       " '20220823180000.npy',\n",
       " '20220824000000.npy',\n",
       " '20220824060000.npy',\n",
       " '20220824120000.npy',\n",
       " '20220824180000.npy',\n",
       " '20220825000000.npy',\n",
       " '20220825060000.npy',\n",
       " '20220825120000.npy',\n",
       " '20220825180000.npy',\n",
       " '20220826000000.npy',\n",
       " '20220826060000.npy',\n",
       " '20220826120000.npy',\n",
       " '20220826180000.npy',\n",
       " '20220827000000.npy',\n",
       " '20220827060000.npy',\n",
       " '20220827120000.npy',\n",
       " '20220827180000.npy',\n",
       " '20220828000000.npy',\n",
       " '20220828060000.npy',\n",
       " '20220828120000.npy',\n",
       " '20220828180000.npy',\n",
       " '20220829000000.npy',\n",
       " '20220829060000.npy',\n",
       " '20220829120000.npy',\n",
       " '20220829180000.npy',\n",
       " '20220830000000.npy',\n",
       " '20220830060000.npy',\n",
       " '20220830120000.npy',\n",
       " '20220830180000.npy',\n",
       " '20220831000000.npy',\n",
       " '20220831060000.npy',\n",
       " '20220831120000.npy',\n",
       " '20220831180000.npy',\n",
       " '20220901000000.npy',\n",
       " '20220901060000.npy',\n",
       " '20220901120000.npy',\n",
       " '20220901180000.npy',\n",
       " '20220902000000.npy',\n",
       " '20220902060000.npy',\n",
       " '20220902120000.npy',\n",
       " '20220902180000.npy',\n",
       " '20220903000000.npy',\n",
       " '20220903060000.npy',\n",
       " '20220903120000.npy',\n",
       " '20220903180000.npy',\n",
       " '20220904000000.npy',\n",
       " '20220904060000.npy',\n",
       " '20220904120000.npy',\n",
       " '20220904180000.npy',\n",
       " '20220905000000.npy',\n",
       " '20220905060000.npy',\n",
       " '20220905120000.npy',\n",
       " '20220905180000.npy',\n",
       " '20220906000000.npy',\n",
       " '20220906060000.npy',\n",
       " '20220906120000.npy',\n",
       " '20220906180000.npy',\n",
       " '20220907000000.npy',\n",
       " '20220907060000.npy',\n",
       " '20220907120000.npy',\n",
       " '20220907180000.npy',\n",
       " ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_paths = train_set.datasets[1].sample_names\n",
    "sample_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([65, 268, 238, 18])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meps_sample_path = \"data/meps_example/samples/train/nwp_2022040100_mbr000.npy\"\n",
    "full_sample = torch.tensor(\n",
    "    np.load(meps_sample_path), dtype=torch.float32\n",
    ")  # (N_t', dim_x, dim_y, d_features')\n",
    "full_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Time Resolution Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ERA5UKDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m n_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m      4\u001b[0m subsample_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m----> 6\u001b[0m train_set \u001b[38;5;241m=\u001b[39m \u001b[43mERA5UKDataset\u001b[49m(\n\u001b[1;32m      7\u001b[0m     dataset_name,\n\u001b[1;32m      8\u001b[0m     pred_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      9\u001b[0m     subsample_step\u001b[38;5;241m=\u001b[39msubsample_step,\n\u001b[1;32m     10\u001b[0m     split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     standardize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m     14\u001b[0m     train_set,\n\u001b[1;32m     15\u001b[0m     batch_size,\n\u001b[1;32m     16\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     17\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39mn_workers,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_batches:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(train_loader))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ERA5UKDataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset_name = \"era5_uk\"\n",
    "batch_size = 4\n",
    "n_workers = 4\n",
    "subsample_step = 2\n",
    "\n",
    "train_set = ERA5UKDataset(\n",
    "    dataset_name,\n",
    "    pred_length=1,\n",
    "    subsample_step=subsample_step,\n",
    "    split=\"train\",\n",
    "    standardize=False,\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=n_workers,\n",
    ")\n",
    "\n",
    "print(\"training_batches:\", len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_set batches: 33\n"
     ]
    }
   ],
   "source": [
    "val_set = ERA5UKDataset(\n",
    "    dataset_name,\n",
    "    pred_length=28,\n",
    "    subsample_step=subsample_step,\n",
    "    split=\"val\",\n",
    "    standardize=False,\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_set,\n",
    "    batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=n_workers,\n",
    ")\n",
    "\n",
    "print(\"validation_set batches:\", len(val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Multi Resolution Dataset + Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from neural_lam.era5_dataset import ERA5MultiResolutionDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"era5_uk_small\", \"era5_uk_big_coarse\"]\n",
    "train_set = ERA5MultiResolutionDataset(\n",
    "    datasets,\n",
    "    split=\"train\",\n",
    ")\n",
    "\n",
    "batch_size = 4\n",
    "n_workers = 4\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=n_workers,\n",
    ")\n",
    "dataiter = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_states, target_states, forcing = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 1840, 48])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_states[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import math\n",
    "\n",
    "from neural_lam import utils, constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestERA5Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    ERA5 UK dataset\n",
    "    \n",
    "    N_t' = 65\n",
    "    N_t = 65//subsample_step (= 21 for 3h steps)\n",
    "    N_x = 268 (width)\n",
    "    N_y = 238 (height)\n",
    "    N_grid = 268x238 = 63784 (total number of grid nodes)\n",
    "    d_features = 17 (d_features' = 18)\n",
    "    d_forcing = 5\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_name,\n",
    "        subsample_steps=[2, 1],\n",
    "        pattern=\"*.npy\",\n",
    "        pred_length=28, \n",
    "        split=\"train\", \n",
    "        year=2022,\n",
    "        month=None,\n",
    "        subsample_step=1,\n",
    "        standardize=False,\n",
    "        subset=False,\n",
    "        control_only=False,\n",
    "        args=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert split in (\"train\", \"val\", \"test\"), \"Unknown dataset split\"\n",
    "        self.sample_dir_path = os.path.join(\"data\", dataset_name, \"samples\", split)\n",
    "        self.args = args\n",
    "        self.split = split\n",
    "        \n",
    "        pattern = f\"{year}{pattern}\"\n",
    "        if self.split == \"train\":\n",
    "            sample_paths = glob.glob(os.path.join(self.sample_dir_path, pattern))\n",
    "            # example name: '20200101000000.npy'\n",
    "            self.sample_names = [os.path.basename(path) for path in sample_paths] \n",
    "            self.sample_names.sort()\n",
    "            self.sample_times = [dt.datetime.strptime(n, '%Y%m%d%H%M%S.npy') for n in self.sample_names]\n",
    "\n",
    "        else:\n",
    "            assert month is not None, \"Month must be specified for validation/test dataset\"\n",
    "            month_dir = os.path.join(self.sample_dir_path, month)\n",
    "            sample_paths = glob.glob(os.path.join(month_dir, pattern))\n",
    "            self.sample_names = [os.path.join(month, os.path.basename(path)) for path in sample_paths]\n",
    "            self.sample_names.sort()\n",
    "            self.sample_times = [dt.datetime.strptime(n[3:], '%Y%m%d%H%M%S.npy') for n in self.sample_names]\n",
    "\n",
    "        if subset:\n",
    "            self.sample_names = self.sample_names[:50] # Limit to 50 samples\n",
    "        \n",
    "        # 2 init states, pred_length target states\n",
    "        self.subsample_steps = [4, 2, 1]\n",
    "        self.pred_length = pred_length\n",
    "        self.sample_length = pred_length + 2 * self.subsample_steps[0]\n",
    "        self.length = len(self.sample_names) - self.sample_length + 1\n",
    "        \n",
    "        print(\"pred_length:\", self.pred_length)\n",
    "        print(\"sample_length\", self.sample_length)\n",
    "        print(\"length:\", self.length)\n",
    "        \n",
    "        assert (\n",
    "            self.length > 0\n",
    "        ), \"Requesting too long time series samples\"\n",
    "\n",
    "        # Set up for standardization\n",
    "        self.standardize = standardize\n",
    "        if standardize:\n",
    "            ds_stats = utils.load_dataset_stats(dataset_name, \"cpu\")\n",
    "            self.data_mean, self.data_std = (\n",
    "                ds_stats[\"data_mean\"],\n",
    "                ds_stats[\"data_std\"],\n",
    "            )\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def _get_sample(self, sample_name):\n",
    "        sample_path = os.path.join(self.sample_dir_path, f\"{sample_name}\")\n",
    "        try:\n",
    "            full_sample = torch.tensor(np.load(sample_path),\n",
    "                    dtype=torch.float32) # (N_lon*N_lat, N_vars*N_levels)\n",
    "        except ValueError:\n",
    "            print(f\"Failed to load {sample_path}\")\n",
    "        \n",
    "        return full_sample\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        _init_states = []\n",
    "        _target_states = []\n",
    "        _forcing_features = []\n",
    "        \n",
    "        _start_idx = idx * self.subsample_steps[0]\n",
    "        _end_idx = _start_idx + self.sample_length\n",
    "        for i in range(len(self.subsample_steps)):\n",
    "            subsample_step = self.subsample_steps[i]\n",
    "            \n",
    "            if i == 0:\n",
    "                start_idx = _start_idx\n",
    "            else:\n",
    "                start_idx = _start_idx + 2 * self.subsample_steps[0] - 2 * subsample_step\n",
    "            \n",
    "            # print(\"idx:\", idx)\n",
    "            # print(\"subsample_step:\", subsample_step)\n",
    "            # print(\"start_idx:\", start_idx)\n",
    "            # print()\n",
    "            \n",
    "            # === Sample ===\n",
    "            prev_prev_state = self._get_sample(self.sample_names[start_idx])\n",
    "            prev_state = self._get_sample(self.sample_names[start_idx+subsample_step])        \n",
    "\n",
    "            # N_grid = N_x * N_y; d_features = N_vars * N_levels\n",
    "            init_states = torch.stack((prev_prev_state, prev_state), dim=0) # (2, N_grid, d_features)\n",
    "            \n",
    "            target_states = []\n",
    "            for i in range(start_idx + 2 * subsample_step, _end_idx, subsample_step):\n",
    "                target_states.append(self._get_sample(self.sample_names[start_idx + i]))\n",
    "            target_states = torch.stack(target_states, dim=0) # (sample_len-2, N_grid, d_features)\n",
    "            \n",
    "            if self.standardize:\n",
    "                # Standardize sample\n",
    "                init_states = (init_states - self.data_mean) / self.data_std\n",
    "                target_states = (target_states - self.data_mean) / self.data_std\n",
    "            \n",
    "            # === Forcing features ===\n",
    "            # Each step is 6 hours long\n",
    "            hour_inc = torch.arange(len(target_states) + 2) * 6 * subsample_step # (sample_len,)\n",
    "            init_dt = self.sample_times[start_idx]\n",
    "            \n",
    "            init_hour = init_dt.hour\n",
    "            hour_of_day = init_hour + hour_inc\n",
    "\n",
    "            start_of_year = dt.datetime(init_dt.year, 1, 1)\n",
    "            init_seconds_into_year = (init_dt - start_of_year).total_seconds()\n",
    "            seconds_into_year = init_seconds_into_year + hour_inc * 3600\n",
    "\n",
    "            hour_angle = (hour_of_day / 24) * 2 * torch.pi \n",
    "            year_angle = (seconds_into_year / constants.SECONDS_IN_YEAR) * 2 * torch.pi\n",
    "            \n",
    "            datetime_forcing = torch.stack(\n",
    "                (\n",
    "                    torch.sin(hour_angle),\n",
    "                    torch.cos(hour_angle),\n",
    "                    torch.sin(year_angle),\n",
    "                    torch.cos(year_angle),\n",
    "                ),\n",
    "                dim=1,\n",
    "            )  # (sample_len, 4)\n",
    "            datetime_forcing = (datetime_forcing + 1) / 2 # Normalize to [0,1]\n",
    "            datetime_forcing = datetime_forcing.unsqueeze(1).expand(\n",
    "                -1, init_states.shape[1], -1\n",
    "            )  # (sample_len, N_grid, 4)\n",
    "\n",
    "            forcing = torch.cat(\n",
    "                (\n",
    "                    datetime_forcing[:-2],\n",
    "                    datetime_forcing[1:-1],\n",
    "                    datetime_forcing[2:],\n",
    "                ),\n",
    "                dim=2,\n",
    "            ) # (sample_len-2, N_grid, 12)\n",
    "            \n",
    "            if self.args and self.args.no_forcing:\n",
    "                forcing = torch.zeros(target_states.shape[0], target_states.shape[1], 0) # (sample_len-2, N_grid, d_forcing)\n",
    "                \n",
    "            _init_states.append(init_states)\n",
    "            _target_states.append(target_states)\n",
    "            _forcing_features.append(forcing)\n",
    "        \n",
    "        return _init_states, _target_states, _forcing_features\n",
    "    \n",
    "def era5_dataset(\n",
    "    dataset_name,\n",
    "    pattern=\"*.npy\",\n",
    "    pred_length=28, \n",
    "    split=\"train\", \n",
    "    year=2022,\n",
    "    subsample_step=1,\n",
    "    standardize=False,\n",
    "    subset=False,\n",
    "    control_only=False,\n",
    "    args=None,\n",
    "):\n",
    "    if split == \"train\":\n",
    "        return TestERA5Dataset(\n",
    "            dataset_name,\n",
    "            pattern=pattern,\n",
    "            pred_length=pred_length, \n",
    "            split=split, \n",
    "            year=\"2022\",\n",
    "            subsample_step=subsample_step,\n",
    "            standardize=standardize,\n",
    "            subset=subset,\n",
    "            control_only=control_only,\n",
    "            args=args,\n",
    "        )\n",
    "    else:\n",
    "        datasets = []\n",
    "        for month in constants.ERA5UKConstants.VAL_MONTHS:\n",
    "            datasets.append(\n",
    "                TestERA5Dataset(\n",
    "                    dataset_name,\n",
    "                    pattern=pattern,\n",
    "                    pred_length=pred_length, \n",
    "                    split=split, \n",
    "                    year=\"2023\",\n",
    "                    month=month,\n",
    "                    subsample_step=subsample_step,\n",
    "                    standardize=standardize,\n",
    "                    subset=subset,\n",
    "                    control_only=control_only,\n",
    "                    args=args,\n",
    "                )\n",
    "            )\n",
    "        return torch.utils.data.ConcatDataset(datasets)\n",
    "    \n",
    "import neural_lam.era5_dataset as nl_era5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_length: 28\n",
      "sample_length 36\n",
      "length: 1425\n",
      "train batches : 713\n"
     ]
    }
   ],
   "source": [
    "from neural_lam.era5_dataset import ERA5MultiTimeDataset\n",
    "\n",
    "dataset_name = \"era5_uk\"\n",
    "batch_size = 2\n",
    "n_workers = 4\n",
    "split = \"train\"\n",
    "\n",
    "\n",
    "train_set = ERA5MultiTimeDataset(\n",
    "    dataset_name,\n",
    "    subsample_steps=[4, 2, 1],\n",
    "    pred_length=28,\n",
    "    split=split,\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=n_workers,\n",
    ")\n",
    "\n",
    "# init_states, target_states, forcing = train_set[0]\n",
    "\n",
    "print(f\"{split} batches :\", len(train_loader))\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "init_states, target_states, forcing = next(dataiter)\n",
    "\n",
    "# print(\"init_states:\", init_states.shape)\n",
    "# print(\"target_states:\", target_states.shape)\n",
    "# print(\"forcing:\", forcing.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3705, 48])\n",
      "torch.Size([2, 2, 3705, 48])\n",
      "torch.Size([2, 2, 3705, 48])\n",
      "torch.Size([2, 7, 3705, 48])\n",
      "torch.Size([2, 14, 3705, 48])\n",
      "torch.Size([2, 28, 3705, 48])\n",
      "torch.Size([2, 7, 3705, 12])\n",
      "torch.Size([2, 14, 3705, 12])\n",
      "torch.Size([2, 28, 3705, 12])\n"
     ]
    }
   ],
   "source": [
    "print(init_states[0].shape)\n",
    "print(init_states[1].shape)\n",
    "print(init_states[2].shape)\n",
    "\n",
    "print(target_states[0].shape)\n",
    "print(target_states[1].shape)\n",
    "print(target_states[2].shape)\n",
    "\n",
    "print(forcing[0].shape)\n",
    "print(forcing[1].shape)\n",
    "print(forcing[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(init_states[1][:, 1] == init_states[2][:, 0]).all()\n",
    "\n",
    "(target_states[0][:, 1] == target_states[1][:, 0]).all()\n",
    "\n",
    "(target_states[0][:, -1] == target_states[1][:, -4]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val batches : 47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_states: torch.Size([8, 2, 3705, 48])\n",
      "target_states: torch.Size([8, 28, 3705, 48])\n",
      "forcing: torch.Size([8, 28, 3705, 12])\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"era5_uk\"\n",
    "batch_size = 8\n",
    "n_workers = 4\n",
    "\n",
    "subsample_step = 1\n",
    "# split, year = \"val\", \"2022\"\n",
    "split = \"val\"\n",
    "\n",
    "train_set = nl_era5.era5_dataset(\n",
    "    dataset_name,\n",
    "    pred_length=28,\n",
    "    subsample_step=subsample_step,\n",
    "    split=split,\n",
    "    standardize=False,\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=n_workers,\n",
    ")\n",
    "\n",
    "print(f\"{split} batches :\", len(train_loader))\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "init_states, target_states, forcing = next(dataiter)\n",
    "print(\"init_states:\", init_states.shape)\n",
    "print(\"target_states:\", target_states.shape)\n",
    "print(\"forcing:\", forcing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val batches : 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_states: torch.Size([8, 2, 3705, 48])\n",
      "target_states: torch.Size([8, 28, 3705, 48])\n",
      "forcing: torch.Size([8, 28, 3705, 12])\n"
     ]
    }
   ],
   "source": [
    "subsample_step = 2\n",
    "# split, year = \"val\", \"2022\"\n",
    "split = \"val\"\n",
    "\n",
    "train_set = nl_era5.era5_dataset(\n",
    "    dataset_name,\n",
    "    pred_length=28,\n",
    "    subsample_step=subsample_step,\n",
    "    split=split,\n",
    "    standardize=False,\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=n_workers,\n",
    ")\n",
    "\n",
    "print(f\"{split} batches :\", len(train_loader))\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "subsampled_init_states, subsampled_target_states, subsampled_forcing = next(dataiter)\n",
    "print(\"init_states:\", subsampled_init_states.shape)\n",
    "print(\"target_states:\", subsampled_target_states.shape)\n",
    "print(\"forcing:\", subsampled_forcing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (subsampled_init_states[:, 1] == target_states[:, 0]).all()\n",
    "(subsampled_target_states[:, 0] == target_states[:, 2]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.9002e+05,  1.2574e+05,  9.5088e+04,  ..., -1.7236e-01,\n",
       "            1.0889e-01, -5.6854e-02],\n",
       "          [ 1.9003e+05,  1.2575e+05,  9.5097e+04,  ..., -1.4230e-01,\n",
       "            1.4538e-01, -5.3848e-02],\n",
       "          [ 1.9004e+05,  1.2576e+05,  9.5110e+04,  ..., -1.5991e-01,\n",
       "            1.1404e-01, -4.6549e-02],\n",
       "          ...,\n",
       "          [ 1.9874e+05,  1.3382e+05,  1.0307e+05,  ..., -2.2074e-02,\n",
       "           -6.2407e-01,  2.8299e-03],\n",
       "          [ 1.9878e+05,  1.3383e+05,  1.0311e+05,  ...,  1.5282e-02,\n",
       "           -7.3656e-01,  1.6570e-02],\n",
       "          [ 1.9882e+05,  1.3384e+05,  1.0313e+05,  ...,  4.7915e-02,\n",
       "           -3.9607e-01, -6.2436e-02]],\n",
       "\n",
       "         [[ 1.9083e+05,  1.2599e+05,  9.5139e+04,  ...,  2.3899e-01,\n",
       "            2.8021e-01,  3.7610e-02],\n",
       "          [ 1.9084e+05,  1.2600e+05,  9.5177e+04,  ...,  1.7759e-01,\n",
       "            2.5359e-01,  4.9632e-02],\n",
       "          [ 1.9085e+05,  1.2601e+05,  9.5212e+04,  ...,  1.2606e-01,\n",
       "            2.0679e-01,  4.8344e-02],\n",
       "          ...,\n",
       "          [ 1.9870e+05,  1.3360e+05,  1.0303e+05,  ...,  7.8830e-02,\n",
       "           -4.6906e-01,  2.9881e-02],\n",
       "          [ 1.9873e+05,  1.3366e+05,  1.0308e+05,  ..., -6.1148e-02,\n",
       "           -3.9306e-01,  7.9689e-02],\n",
       "          [ 1.9875e+05,  1.3371e+05,  1.0313e+05,  ..., -1.6377e-01,\n",
       "           -2.8786e-01, -4.5690e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 1.9045e+05,  1.2583e+05,  9.5062e+04,  ..., -6.3051e-01,\n",
       "           -1.5304e-01,  9.7000e-03],\n",
       "          [ 1.9045e+05,  1.2584e+05,  9.5094e+04,  ..., -7.6362e-01,\n",
       "           -3.9950e-01,  1.3135e-02],\n",
       "          [ 1.9046e+05,  1.2585e+05,  9.5129e+04,  ..., -8.4949e-01,\n",
       "           -6.8933e-01,  1.3135e-02],\n",
       "          ...,\n",
       "          [ 1.9861e+05,  1.3350e+05,  1.0299e+05,  ...,  1.6900e-01,\n",
       "           -5.0470e-01, -5.7577e-03],\n",
       "          [ 1.9863e+05,  1.3356e+05,  1.0303e+05,  ...,  2.3126e-01,\n",
       "           -5.9358e-01, -1.4639e-03],\n",
       "          [ 1.9865e+05,  1.3362e+05,  1.0307e+05,  ...,  3.1027e-01,\n",
       "           -3.3123e-01, -9.4640e-02]],\n",
       "\n",
       "         [[ 1.9127e+05,  1.2616e+05,  9.5107e+04,  ...,  3.4175e-02,\n",
       "           -3.7961e-02,  2.2581e-02],\n",
       "          [ 1.9128e+05,  1.2617e+05,  9.5136e+04,  ...,  3.1169e-02,\n",
       "           -2.6368e-02,  3.4604e-02],\n",
       "          [ 1.9129e+05,  1.2618e+05,  9.5164e+04,  ...,  3.4175e-02,\n",
       "            1.3564e-02,  4.1045e-02],\n",
       "          ...,\n",
       "          [ 1.9876e+05,  1.3356e+05,  1.0289e+05,  ...,  1.1447e-01,\n",
       "           -4.0594e-01,  3.1169e-02],\n",
       "          [ 1.9877e+05,  1.3362e+05,  1.0295e+05,  ...,  1.0717e-01,\n",
       "           -3.8447e-01,  8.4842e-02],\n",
       "          [ 1.9879e+05,  1.3366e+05,  1.0301e+05,  ...,  1.8489e-01,\n",
       "           -1.8567e-01, -6.2865e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 1.9083e+05,  1.2599e+05,  9.5139e+04,  ...,  2.3899e-01,\n",
       "            2.8021e-01,  3.7610e-02],\n",
       "          [ 1.9084e+05,  1.2600e+05,  9.5177e+04,  ...,  1.7759e-01,\n",
       "            2.5359e-01,  4.9632e-02],\n",
       "          [ 1.9085e+05,  1.2601e+05,  9.5212e+04,  ...,  1.2606e-01,\n",
       "            2.0679e-01,  4.8344e-02],\n",
       "          ...,\n",
       "          [ 1.9870e+05,  1.3360e+05,  1.0303e+05,  ...,  7.8830e-02,\n",
       "           -4.6906e-01,  2.9881e-02],\n",
       "          [ 1.9873e+05,  1.3366e+05,  1.0308e+05,  ..., -6.1148e-02,\n",
       "           -3.9306e-01,  7.9689e-02],\n",
       "          [ 1.9875e+05,  1.3371e+05,  1.0313e+05,  ..., -1.6377e-01,\n",
       "           -2.8786e-01, -4.5690e-02]],\n",
       "\n",
       "         [[ 1.9157e+05,  1.2647e+05,  9.5276e+04,  ...,  1.5784e-01,\n",
       "            3.0168e-01,  1.3135e-02],\n",
       "          [ 1.9158e+05,  1.2649e+05,  9.5289e+04,  ...,  1.4968e-01,\n",
       "            2.6046e-01,  1.7429e-02],\n",
       "          [ 1.9159e+05,  1.2650e+05,  9.5305e+04,  ...,  1.3594e-01,\n",
       "            2.2224e-01,  2.3011e-02],\n",
       "          ...,\n",
       "          [ 1.9889e+05,  1.3342e+05,  1.0269e+05,  ...,  1.0245e-01,\n",
       "           -8.9458e-01, -5.7577e-03],\n",
       "          [ 1.9893e+05,  1.3349e+05,  1.0274e+05,  ...,  1.1318e-01,\n",
       "           -9.7616e-01,  4.3621e-02],\n",
       "          [ 1.9897e+05,  1.3355e+05,  1.0280e+05,  ...,  8.1836e-02,\n",
       "           -7.2411e-01, -6.5012e-02]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 1.9189e+05,  1.2664e+05,  9.5504e+04,  ...,  1.6041e-01,\n",
       "            2.9009e-01,  2.4299e-02],\n",
       "          [ 1.9189e+05,  1.2665e+05,  9.5510e+04,  ...,  1.5569e-01,\n",
       "            3.5106e-01,  2.4728e-02],\n",
       "          [ 1.9190e+05,  1.2666e+05,  9.5520e+04,  ...,  1.4496e-01,\n",
       "            3.6652e-01,  2.9881e-02],\n",
       "          ...,\n",
       "          [ 1.9879e+05,  1.3287e+05,  1.0207e+05,  ..., -6.4596e-01,\n",
       "           -9.5125e-01,  2.8163e-02],\n",
       "          [ 1.9879e+05,  1.3293e+05,  1.0212e+05,  ..., -5.7984e-01,\n",
       "           -1.0603e+00,  3.8898e-02],\n",
       "          [ 1.9881e+05,  1.3299e+05,  1.0217e+05,  ..., -2.0671e-01,\n",
       "           -8.9801e-01, -5.8142e-02]],\n",
       "\n",
       "         [[ 1.9257e+05,  1.2717e+05,  9.6253e+04,  ...,  4.3192e-02,\n",
       "            6.2943e-02, -5.1272e-02],\n",
       "          [ 1.9258e+05,  1.2718e+05,  9.6256e+04,  ...,  1.6141e-02,\n",
       "            7.7542e-02, -6.4154e-02],\n",
       "          [ 1.9258e+05,  1.2719e+05,  9.6262e+04,  ..., -1.8932e-03,\n",
       "            1.5870e-01, -4.9984e-02],\n",
       "          ...,\n",
       "          [ 1.9907e+05,  1.3243e+05,  1.0147e+05,  ..., -7.5317e-02,\n",
       "            3.2787e-01,  6.2650e-03],\n",
       "          [ 1.9906e+05,  1.3246e+05,  1.0153e+05,  ..., -1.0323e-01,\n",
       "            3.5965e-01,  9.2707e-03],\n",
       "          [ 1.9905e+05,  1.3250e+05,  1.0158e+05,  ..., -1.7622e-01,\n",
       "            3.4376e-01, -4.7837e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 1.9223e+05,  1.2695e+05,  9.5917e+04,  ...,  3.7510e-01,\n",
       "            7.1238e-03, -2.1215e-02],\n",
       "          [ 1.9224e+05,  1.2696e+05,  9.5920e+04,  ...,  3.9657e-01,\n",
       "            2.6017e-02, -2.6797e-02],\n",
       "          [ 1.9225e+05,  1.2697e+05,  9.5926e+04,  ...,  3.9915e-01,\n",
       "            5.3926e-02, -1.2628e-02],\n",
       "          ...,\n",
       "          [ 1.9917e+05,  1.3270e+05,  1.0174e+05,  ..., -6.8032e-01,\n",
       "            3.7038e-01, -5.5566e-02],\n",
       "          [ 1.9914e+05,  1.3275e+05,  1.0179e+05,  ..., -5.7812e-01,\n",
       "            3.8670e-01, -1.1310e-01],\n",
       "          [ 1.9910e+05,  1.3278e+05,  1.0184e+05,  ..., -3.4368e-01,\n",
       "            3.0254e-01, -9.5498e-02]],\n",
       "\n",
       "         [[ 1.9301e+05,  1.2752e+05,  9.6586e+04,  ...,  1.9949e-01,\n",
       "           -2.8944e-02, -5.9860e-02],\n",
       "          [ 1.9301e+05,  1.2752e+05,  9.6605e+04,  ...,  1.7286e-01,\n",
       "           -1.0051e-02, -6.2007e-02],\n",
       "          [ 1.9300e+05,  1.2753e+05,  9.6627e+04,  ...,  1.3937e-01,\n",
       "            6.4231e-02, -4.7837e-02],\n",
       "          ...,\n",
       "          [ 1.9913e+05,  1.3250e+05,  1.0126e+05,  ...,  3.5892e-02,\n",
       "           -1.3916e-02,  6.1226e-02],\n",
       "          [ 1.9914e+05,  1.3253e+05,  1.0131e+05,  ...,  4.6197e-02,\n",
       "            9.2141e-02,  5.9938e-02],\n",
       "          [ 1.9914e+05,  1.3255e+05,  1.0135e+05,  ...,  2.2581e-02,\n",
       "            3.4604e-02,  2.3440e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 1.9257e+05,  1.2717e+05,  9.6253e+04,  ...,  4.3192e-02,\n",
       "            6.2943e-02, -5.1272e-02],\n",
       "          [ 1.9258e+05,  1.2718e+05,  9.6256e+04,  ...,  1.6141e-02,\n",
       "            7.7542e-02, -6.4154e-02],\n",
       "          [ 1.9258e+05,  1.2719e+05,  9.6262e+04,  ..., -1.8932e-03,\n",
       "            1.5870e-01, -4.9984e-02],\n",
       "          ...,\n",
       "          [ 1.9907e+05,  1.3243e+05,  1.0147e+05,  ..., -7.5317e-02,\n",
       "            3.2787e-01,  6.2650e-03],\n",
       "          [ 1.9906e+05,  1.3246e+05,  1.0153e+05,  ..., -1.0323e-01,\n",
       "            3.5965e-01,  9.2707e-03],\n",
       "          [ 1.9905e+05,  1.3250e+05,  1.0158e+05,  ..., -1.7622e-01,\n",
       "            3.4376e-01, -4.7837e-02]],\n",
       "\n",
       "         [[ 1.9333e+05,  1.2782e+05,  9.6627e+04,  ...,  2.0035e-01,\n",
       "           -8.8628e-02, -4.6120e-02],\n",
       "          [ 1.9333e+05,  1.2783e+05,  9.6650e+04,  ...,  2.1237e-01,\n",
       "           -6.3724e-02, -4.6549e-02],\n",
       "          [ 1.9334e+05,  1.2783e+05,  9.6672e+04,  ...,  2.4157e-01,\n",
       "           -6.4583e-02, -3.8820e-02],\n",
       "          ...,\n",
       "          [ 1.9916e+05,  1.3251e+05,  1.0109e+05,  ...,  4.1045e-02,\n",
       "            1.5183e-01,  5.5214e-02],\n",
       "          [ 1.9915e+05,  1.3252e+05,  1.0112e+05,  ...,  3.9327e-02,\n",
       "            2.2010e-01,  6.6378e-02],\n",
       "          [ 1.9915e+05,  1.3253e+05,  1.0116e+05,  ..., -6.0510e-04,\n",
       "            2.7420e-01,  4.4480e-02]]]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_sampled_init_states"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
