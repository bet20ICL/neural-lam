{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from neural_lam.era5_dataset import era5_multi_time_dataset, era5_dataset\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import math\n",
    "\n",
    "from neural_lam import utils, constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Dataset with more time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batches : 182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_states: torch.Size([8, 2, 3705, 48])\n",
      "target_states: torch.Size([8, 1, 3705, 48])\n",
      "forcing: torch.Size([8, 1, 3705, 12])\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"era5_uk\"\n",
    "batch_size = 8\n",
    "n_workers = 4\n",
    "\n",
    "subsample_step = 4\n",
    "# split, year = \"val\", \"2022\"\n",
    "split = \"train\"\n",
    "\n",
    "train_set = era5_dataset(\n",
    "    dataset_name,\n",
    "    pred_length=1,\n",
    "    subsample_step=subsample_step,\n",
    "    split=split,\n",
    "    standardize=False,\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=n_workers,\n",
    ")\n",
    "\n",
    "print(f\"{split} batches :\", len(train_loader))\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "init_states, target_states, forcing = next(dataiter)\n",
    "print(\"init_states:\", init_states.shape)\n",
    "print(\"target_states:\", target_states.shape)\n",
    "print(\"forcing:\", forcing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1460"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "365 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 28, 3705, 48])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_step_target_states = target_states.clone()\n",
    "single_step_target_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batches : 182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_states: torch.Size([8, 2, 3705, 48])\n",
      "target_states: torch.Size([8, 1, 3705, 48])\n",
      "forcing: torch.Size([8, 1, 3705, 12])\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"era5_uk\"\n",
    "batch_size = 8\n",
    "n_workers = 4\n",
    "\n",
    "subsample_step = 4\n",
    "# split, year = \"val\", \"2022\"\n",
    "split = \"train\"\n",
    "\n",
    "train_set = era5_dataset(\n",
    "    dataset_name,\n",
    "    pred_length=1,\n",
    "    subsample_step=subsample_step,\n",
    "    split=split,\n",
    "    standardize=False,\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=n_workers,\n",
    ")\n",
    "\n",
    "print(f\"{split} batches :\", len(train_loader))\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "init_states, target_states, forcing = next(dataiter)\n",
    "print(\"init_states:\", init_states.shape)\n",
    "print(\"target_states:\", target_states.shape)\n",
    "print(\"forcing:\", forcing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiTimeDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ERA5MultiTimeDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    ERA5 UK dataset\n",
    "    \n",
    "    N_t' = 65\n",
    "    N_t = 65//subsample_step (= 21 for 3h steps)\n",
    "    N_x = 268 (width)\n",
    "    N_y = 238 (height)\n",
    "    N_grid = 268x238 = 63784 (total number of grid nodes)\n",
    "    d_features = 17 (d_features' = 18)\n",
    "    d_forcing = 5\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_name,\n",
    "        subsample_steps=[2, 1],\n",
    "        pattern=\"*.npy\",\n",
    "        pred_length=28, \n",
    "        split=\"train\", \n",
    "        year=2022,\n",
    "        month=None,\n",
    "        standardize=False,\n",
    "        subset=False,\n",
    "        control_only=False,\n",
    "        args=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert split in (\"train\", \"val\", \"test\"), \"Unknown dataset split\"\n",
    "        self.sample_dir_path = os.path.join(\"data\", dataset_name, \"samples\", split)\n",
    "        self.args = args\n",
    "        self.split = split\n",
    "        \n",
    "        pattern = f\"{year}{pattern}\"\n",
    "        if self.split == \"train\":\n",
    "            sample_paths = glob.glob(os.path.join(self.sample_dir_path, pattern))\n",
    "            # example name: '20200101000000.npy'\n",
    "            self.sample_names = [os.path.basename(path) for path in sample_paths] \n",
    "            self.sample_names.sort()\n",
    "            self.sample_times = [dt.datetime.strptime(n, '%Y%m%d%H%M%S.npy') for n in self.sample_names]\n",
    "\n",
    "        else:\n",
    "            assert month is not None, \"Month must be specified for validation/test dataset\"\n",
    "            month_dir = os.path.join(self.sample_dir_path, month)\n",
    "            sample_paths = glob.glob(os.path.join(month_dir, pattern))\n",
    "            self.sample_names = [os.path.join(month, os.path.basename(path)) for path in sample_paths]\n",
    "            self.sample_names.sort()\n",
    "            self.sample_times = [dt.datetime.strptime(n[3:], '%Y%m%d%H%M%S.npy') for n in self.sample_names]\n",
    "\n",
    "        if subset:\n",
    "            self.sample_names = self.sample_names[:50] # Limit to 50 samples\n",
    "        \n",
    "        # 2 init states, pred_length target states\n",
    "        self.subsample_steps = subsample_steps\n",
    "        self.pred_length = pred_length\n",
    "        self.sample_length = pred_length + 2 * self.subsample_steps[0]\n",
    "        self.length = len(self.sample_names) - self.sample_length + 1\n",
    "        \n",
    "        assert (\n",
    "            self.length > 0\n",
    "        ), \"Requesting too long time series samples\"\n",
    "\n",
    "        # Set up for standardization\n",
    "        self.standardize = standardize\n",
    "        if standardize:\n",
    "            ds_stats = utils.load_dataset_stats(dataset_name, \"cpu\")\n",
    "            self.data_mean, self.data_std = (\n",
    "                ds_stats[\"data_mean\"],\n",
    "                ds_stats[\"data_std\"],\n",
    "            )\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def _get_sample(self, sample_name):\n",
    "        sample_path = os.path.join(self.sample_dir_path, f\"{sample_name}\")\n",
    "        try:\n",
    "            full_sample = torch.tensor(np.load(sample_path),\n",
    "                    dtype=torch.float32) # (N_lon*N_lat, N_vars*N_levels)\n",
    "        except ValueError:\n",
    "            print(f\"Failed to load {sample_path}\")\n",
    "        \n",
    "        return full_sample\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        _init_states = []\n",
    "        _target_states = []\n",
    "        _forcing_features = []\n",
    "        \n",
    "        _start_idx = idx * self.subsample_steps[0]\n",
    "        _end_idx = _start_idx + self.sample_length\n",
    "        for i in range(len(self.subsample_steps)):\n",
    "            subsample_step = self.subsample_steps[i]\n",
    "            \n",
    "            if i == 0:\n",
    "                start_idx = _start_idx\n",
    "            else:\n",
    "                start_idx = _start_idx + 2 * self.subsample_steps[0] - 2 * subsample_step\n",
    "            \n",
    "            # print(\"idx:\", idx)\n",
    "            # print(\"subsample_step:\", subsample_step)\n",
    "            # print(\"start_idx:\", start_idx)\n",
    "            # print()\n",
    "            \n",
    "            # === Sample ===\n",
    "            prev_prev_state = self._get_sample(self.sample_names[start_idx])\n",
    "            prev_state = self._get_sample(self.sample_names[start_idx+subsample_step])        \n",
    "\n",
    "            # N_grid = N_x * N_y; d_features = N_vars * N_levels\n",
    "            init_states = torch.stack((prev_prev_state, prev_state), dim=0) # (2, N_grid, d_features)\n",
    "            \n",
    "            target_states = []\n",
    "            for i in range(start_idx + 2 * subsample_step, _end_idx, subsample_step):\n",
    "                target_states.append(self._get_sample(self.sample_names[start_idx + i]))\n",
    "            target_states = torch.stack(target_states, dim=0) # (sample_len-2, N_grid, d_features)\n",
    "            \n",
    "            if self.standardize:\n",
    "                # Standardize sample\n",
    "                init_states = (init_states - self.data_mean) / self.data_std\n",
    "                target_states = (target_states - self.data_mean) / self.data_std\n",
    "            \n",
    "            # === Forcing features ===\n",
    "            # Each step is 6 hours long\n",
    "            hour_inc = torch.arange(len(target_states) + 2) * 6 * subsample_step # (sample_len,)\n",
    "            init_dt = self.sample_times[start_idx]\n",
    "            \n",
    "            init_hour = init_dt.hour\n",
    "            hour_of_day = init_hour + hour_inc\n",
    "\n",
    "            start_of_year = dt.datetime(init_dt.year, 1, 1)\n",
    "            init_seconds_into_year = (init_dt - start_of_year).total_seconds()\n",
    "            seconds_into_year = init_seconds_into_year + hour_inc * 3600\n",
    "\n",
    "            hour_angle = (hour_of_day / 24) * 2 * torch.pi \n",
    "            year_angle = (seconds_into_year / constants.SECONDS_IN_YEAR) * 2 * torch.pi\n",
    "            \n",
    "            datetime_forcing = torch.stack(\n",
    "                (\n",
    "                    torch.sin(hour_angle),\n",
    "                    torch.cos(hour_angle),\n",
    "                    torch.sin(year_angle),\n",
    "                    torch.cos(year_angle),\n",
    "                ),\n",
    "                dim=1,\n",
    "            )  # (sample_len, 4)\n",
    "            datetime_forcing = (datetime_forcing + 1) / 2 # Normalize to [0,1]\n",
    "            datetime_forcing = datetime_forcing.unsqueeze(1).expand(\n",
    "                -1, init_states.shape[1], -1\n",
    "            )  # (sample_len, N_grid, 4)\n",
    "\n",
    "            forcing = torch.cat(\n",
    "                (\n",
    "                    datetime_forcing[:-2],\n",
    "                    datetime_forcing[1:-1],\n",
    "                    datetime_forcing[2:],\n",
    "                ),\n",
    "                dim=2,\n",
    "            ) # (sample_len-2, N_grid, 12)\n",
    "            \n",
    "            if self.args and self.args.no_forcing:\n",
    "                forcing = torch.zeros(target_states.shape[0], target_states.shape[1], 0) # (sample_len-2, N_grid, d_forcing)\n",
    "                \n",
    "            _init_states.append(init_states)\n",
    "            _target_states.append(target_states)\n",
    "            _forcing_features.append(forcing)\n",
    "        \n",
    "        return _init_states, [_target_states[-1]], _forcing_features\n",
    "    \n",
    "    \n",
    "def era5_multi_time_dataset(\n",
    "    dataset_name,\n",
    "    pattern=\"*.npy\",\n",
    "    pred_length=28, \n",
    "    subsample_steps=[2, 1],\n",
    "    split=\"train\", \n",
    "    year=2022,\n",
    "    standardize=False,\n",
    "    subset=False,\n",
    "    control_only=False,\n",
    "    args=None,\n",
    "):\n",
    "    if split == \"train\":\n",
    "        return ERA5MultiTimeDataset(\n",
    "            dataset_name,\n",
    "            subsample_steps=subsample_steps,\n",
    "            pattern=pattern,\n",
    "            pred_length=pred_length, \n",
    "            split=split, \n",
    "            year=\"2022\",\n",
    "            standardize=standardize,\n",
    "            subset=subset,\n",
    "            control_only=control_only,\n",
    "            args=args,\n",
    "        )\n",
    "    else:\n",
    "        datasets = []\n",
    "        for month in constants.ERA5UKConstants.VAL_MONTHS:\n",
    "            datasets.append(\n",
    "                ERA5MultiTimeDataset(\n",
    "                    dataset_name,\n",
    "                    subsample_steps=subsample_steps,\n",
    "                    pattern=pattern,\n",
    "                    pred_length=pred_length, \n",
    "                    split=split, \n",
    "                    year=\"2023\",\n",
    "                    month=month,\n",
    "                    standardize=standardize,\n",
    "                    subset=subset,\n",
    "                    control_only=control_only,\n",
    "                    args=args,\n",
    "                )\n",
    "            )\n",
    "        return torch.utils.data.ConcatDataset(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2, 6561, 48])\n",
      "torch.Size([8, 2, 6561, 48])\n",
      "torch.Size([8, 2, 6561, 48])\n",
      "torch.Size([8, 1, 6561, 48])\n",
      "torch.Size([8, 1, 6561, 12])\n",
      "torch.Size([8, 1, 6561, 12])\n",
      "torch.Size([8, 1, 6561, 12])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = \"era5_uk_big\"\n",
    "resolutions = [4, 2, 1]\n",
    "standardize = True\n",
    "batch_size = 8\n",
    "num_workers = 4\n",
    "\n",
    "val_set = era5_multi_time_dataset(\n",
    "    dataset,\n",
    "    subsample_steps=resolutions,\n",
    "    pred_length=1,\n",
    "    split=\"train\",\n",
    "    standardize=bool(standardize),\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_set, batch_size=batch_size, shuffle=False, num_workers=4\n",
    ")\n",
    "\n",
    "data_iter = iter(val_loader)\n",
    "batch = next(data_iter)\n",
    "\n",
    "print(batch[0][0].shape)\n",
    "print(batch[0][1].shape)\n",
    "print(batch[0][2].shape)\n",
    "\n",
    "print(batch[1][0].shape)\n",
    "\n",
    "print(batch[2][0].shape)\n",
    "print(batch[2][1].shape)\n",
    "print(batch[2][2].shape)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# count = 0\n",
    "# for batch in val_loader:\n",
    "#     count += 1\n",
    "#     print(batch[0][0].shape)\n",
    "#     print(batch[0][1].shape)\n",
    "#     print(batch[0][2].shape)\n",
    "\n",
    "#     print(batch[1][0].shape)\n",
    "\n",
    "#     print(batch[2][0].shape)\n",
    "#     print(batch[2][1].shape)\n",
    "#     print(batch[2][2].shape)\n",
    "\n",
    "#     print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
